# -*- coding: utf-8 -*-
"""MLE_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_AvIL_OwTwIXJRwmf9GbFvMB8LSK6O-3
"""

!pip install nltk

!pip install pymupdf

!pip install requests

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

import requests
import fitz  # PyMuPDF
from io import BytesIO
import numpy as np
import pandas as pd

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

train_data_path = '/content/drive/MyDrive/NLP Project/data/mle-1-assign-dataset - train_data.csv'
test_data_path = '/content/drive/MyDrive/NLP Project/data/mle-1-assign-dataset - test_data.csv'

"""## Data Cleaning"""

train_df = pd.read_csv(train_data_path)
test_df = pd.read_csv(test_data_path)

train_data = train_df.copy()
test_data = test_df.copy()

# function for data cleaning
def data_cleaning(df):
  df.drop_duplicates(inplace=True)
  df.drop(df[df['datasheet_link'] == '-'].index,inplace=True)
  return df

# Train data size
print('*'*50)
print('Original Train Data size : ',len(train_data))
clean_train_data = data_cleaning(train_df).copy()
print('Cleaned Train Data size : ',len(clean_train_data))

print('*'*50)

# Test data size
print('Original Test Data size : ',len(test_data))
clean_test_data = data_cleaning(test_df).copy()
print('Cleaned Test Data size : ',len(clean_test_data))
print('*'*50)

# Visualization of our data before cleaning and after cleaning
print('*'*120)
train_values =  dict(train_data['target_col'].value_counts())
clean_train_values = dict(clean_train_data['target_col'].value_counts())
print('Orizinal data values of each labels in train data : ',train_values)
print('Cleaned data values of each labels in train data : ',clean_train_values)

print('*'*120)
test_values =  dict(test_data['target_col'].value_counts())
clean_test_values = dict(clean_test_data['target_col'].value_counts())
print('Orizinal data values of each labels in test data : ',test_values)
print('Cleaned data values of each labels in test data : ',clean_test_values)
print('*'*120)

import matplotlib.pyplot as plt

# Create subplots: 2 rows, 2 columns
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 8))

# Plot the first pie chart
ax1.pie(train_values.values(), labels=train_values.keys(), autopct='%1.1f%%', startangle=90)
ax1.set_title('Unclean Train Data')

# Plot the second pie chart
ax2.pie(clean_train_values.values(), labels=clean_train_values.keys(), autopct='%1.1f%%', startangle=90)
ax2.set_title('Cleaned Train Data')

# Plot the third pie chart
ax3.pie(test_values.values(), labels=test_values.keys(), autopct='%1.1f%%', startangle=90)
ax3.set_title('Unclean Test Data')

# Plot the fourth pie chart
ax4.pie(clean_test_values.values(), labels=clean_test_values.keys(), autopct='%1.1f%%', startangle=90)
ax4.set_title('Clean Test Data')

# Display the plots
plt.show()

"""## Text Extraction and Preprocessing"""

class customdata:
  def __init__(self):
    self.pdf_url = ''
    self.text = ''

# Extract text from PDF File link
  def extract_text_from_pdf(self,pdf_url):
    self.pdf_url = pdf_url
    try:
        # Download the PDF
        response = requests.get(self.pdf_url, timeout=5)
        response.raise_for_status()  # Raise an exception for HTTP errors

        # Read the PDF content
        pdf_content = BytesIO(response.content)
        pdf_document = fitz.open(stream=pdf_content, filetype="pdf")

        # Extract text from each page
        text = ""
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text += page.get_text()

        return text

    except Exception as e:
        print(f"Error processing {pdf_url}: {e}")
        return ""

# Text pre processing
  def preprocess_text(self,text):
    self.text = text
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    self.text = self.text.lower()

    # Remove punctuation and special characters
    self.text = re.sub(r'[^0-9a-zA-Z\s]', '', self.text)

    tokens = word_tokenize(self.text)
    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]

    processed_text = ' '.join(processed_tokens)

    return processed_text

def data_frame(df):
  data_columns = ['text','target']
  new_df = pd.DataFrame(columns=data_columns)
  obj = customdata()

  for index, row in df.iterrows():
    url = row['datasheet_link']
    target = row['target_col']
    pdf_text = obj.extract_text_from_pdf(url)
    if pdf_text != '':
      preprocess_text = obj.preprocess_text(pdf_text)
      print(index,preprocess_text)
      new_row = pd.DataFrame({'text':[preprocess_text],'target': [target]})
      new_df = pd.concat([new_df,new_row], ignore_index=True)
    else:
      continue

  return new_df

test = data_frame(clean_test_data).copy()

train = data_frame(clean_train_data).copy()

#train.to_csv('train.csv', index=False)

#test.to_csv('test.csv', index=False)

"""## Modeling"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB, ComplementNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report

import warnings
# Suppress all warnings
warnings.filterwarnings('ignore')

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

X_train = train['text']
y_train = train['target']
X_test = test['text']
y_test = test['target']

pipeMNB = Pipeline([('tfidf', TfidfVectorizer()),('clf', MultinomialNB())])
pipeCNB = Pipeline([('tfidf', TfidfVectorizer()),('clf', ComplementNB())])
pipeSVC = Pipeline([('tfidf', TfidfVectorizer()),('clf', LinearSVC())])

pipeMNB.fit(X_train,y_train)
predictMNB = pipeMNB.predict(X_test)
print(f'MNB : {accuracy_score(y_test,predictMNB):.2f}')
pipeCNB.fit(X_train,y_train)
predictCNB = pipeCNB.predict(X_test)
print(f'CNB : {accuracy_score(y_test,predictCNB):.2f}')
pipeSVC.fit(X_train,y_train)
predictSVC = pipeSVC.predict(X_test)
print(f'SVC : {accuracy_score(y_test,predictSVC):.2f}')

print(f'MNB:{classification_report(y_test,predictMNB)}')

print(f'CNB:{classification_report(y_test,predictCNB)}')

print(f'SVC:{classification_report(y_test,predictSVC)}')

"""## User Inputs"""

#url = 'https://catalog.belden.com/techdata/EN/1505A_techdata.pdf'                              # cable
#url_1 = 'https://lumenart.com/images/fabric/cyp/cyp_specs.pdf'                                 # lighting
#url_2 = 'https://www.belfuse.com/resources/datasheets/circuitprotection/ds-cp-rst-series.pdf'  # fuses

def user(url):
  obj = customdata()
  pdf_text = obj.extract_text_from_pdf(url)
  if pdf_text != '':
    preprocess_text = obj.preprocess_text(pdf_text)
    predictSVC = pipeSVC.predict([preprocess_text])

    return predictSVC
  else:
    return ['url Error Found']

user_input = input('Please Enter PDF Url : ')
predicetd_target = user(user_input)
print('Classifier result :',predicetd_target[0])

